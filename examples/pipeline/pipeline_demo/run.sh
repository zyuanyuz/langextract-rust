#!/bin/bash
# Pipeline Demo - Demonstrates multi-step pipeline processing

set -e

# Colors for output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
BOLD='\033[1m'
NC='\033[0m' # No Color

# Get script directory
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

echo -e "${BOLD}${PURPLE}ğŸ”¬ LangExtract Pipeline Processing Demo${NC}"
echo "Demonstrating multi-step extraction workflows with dependent processing"
echo

# Check if lx-rs is available
if command -v lx-rs &> /dev/null; then
    CLI_CMD="lx-rs"
    echo -e "${GREEN}âœ… Using installed lx-rs binary${NC}"
elif [ -f "../../target/release/lx-rs" ]; then
    CLI_CMD="../../target/release/lx-rs"
    echo -e "${GREEN}âœ… Using local release binary${NC}"
elif [ -f "../../../target/release/lx-rs" ]; then
    CLI_CMD="../../../target/release/lx-rs"
    echo -e "${GREEN}âœ… Using local release binary${NC}"
else
    CLI_CMD="cargo run --features=cli --bin lx-rs --"
    echo -e "${YELLOW}âš ï¸  Using cargo run (slower, but works without install)${NC}"
fi

echo

# Show the input document
echo -e "${BLUE}ğŸ“„ Input Document - Technical Requirements:${NC}"
input_text=$(cat "$SCRIPT_DIR/input.txt")
char_count=$(echo -n "$input_text" | wc -c)
word_count=$(echo "$input_text" | wc -w)

echo "   ğŸ“Š Document Statistics:"
echo "      â€¢ Characters: $char_count"
echo "      â€¢ Words: $word_count"
echo "      â€¢ Content: Technical requirements with multiple categories"
echo

# Show preview
echo -e "${BLUE}ğŸ“– Document Preview:${NC}"
echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
head -12 "$SCRIPT_DIR/input.txt"
echo "... (document continues with more requirements)"
echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
echo

# Explain the pipeline concept
echo -e "${BLUE}ğŸ”¬ Pipeline Processing Concept:${NC}"
echo "This demo shows a 3-step pipeline:"
echo "   1ï¸âƒ£  Extract Requirements - Find all 'shall' statements and requirements"
echo "   2ï¸âƒ£  Extract Values - Pull numeric values and units from requirements (parallel)"
echo "   3ï¸âƒ£  Extract Specifications - Extract security and technical specs (parallel)"
echo
echo "Steps 2 and 3 depend on Step 1 and can run in parallel for efficiency."
echo

# Show pipeline configuration
echo -e "${BLUE}ğŸ“‹ Pipeline Configuration:${NC}"
if [ -f "$SCRIPT_DIR/requirements_pipeline.yaml" ]; then
    echo "Pipeline: requirements_pipeline.yaml"
    echo "   â€¢ Enable parallel execution: $(grep 'enable_parallel_execution' "$SCRIPT_DIR/requirements_pipeline.yaml" | cut -d':' -f2 | xargs)"
    echo "   â€¢ Number of steps: $(grep -c '^  - id:' "$SCRIPT_DIR/requirements_pipeline.yaml")"
    echo "   â€¢ Provider: ollama/mistral (configurable)"
fi
echo

# Create output directory
mkdir -p "$SCRIPT_DIR/output"

# Run the pipeline processing
echo -e "${CYAN}ğŸ”„ Running Multi-Step Pipeline...${NC}"
echo

start_time=$(date +%s)

# Note: The CLI currently doesn't have native pipeline support, so we'll simulate
# the pipeline process by running individual extractions that build on each other

echo -e "${CYAN}ğŸ“ Step 1: Extract Requirements${NC}"
echo "Finding all 'shall' statements and requirements..."

$CLI_CMD extract "$SCRIPT_DIR/input.txt" \
    --prompt "Extract ONLY the requirements and 'shall' statements from this text. Focus on system requirements, performance criteria, security rules, and data specifications. Each requirement should be a separate extraction with a descriptive class name." \
    --examples "$SCRIPT_DIR/example_1.json" \
    --provider ollama \
    --model mistral \
    --output "$SCRIPT_DIR/output/step1_requirements.json" \
    --format json \
    --temperature 0.3 \
    --workers 6 \
    --debug > /dev/null 2>&1

echo "   âœ… Requirements extraction completed"

echo
echo -e "${CYAN}ğŸ“ Step 2: Extract Values (Running in Parallel)${NC}"
echo "Extracting numeric values, units, and performance metrics..."

# Step 2 processes the requirements from Step 1, not the original document
echo "   ğŸ“‹ Processing requirements from Step 1..."
if [ -f "$SCRIPT_DIR/output/step1_requirements.json" ]; then
    # Extract the requirement texts to create input for Step 2
    python3 -c "
import json
with open('$SCRIPT_DIR/output/step1_requirements.json', 'r') as f:
    data = json.load(f)
requirements_text = '\n'.join([ext['extraction_text'] for ext in data['extractions']])
with open('$SCRIPT_DIR/output/step1_requirements_text.txt', 'w') as f:
    f.write(requirements_text)
" > /dev/null 2>&1 || true
    
    $CLI_CMD extract "$SCRIPT_DIR/output/step1_requirements_text.txt" \
        --prompt "From these requirement statements, extract ONLY the specific numeric values, percentages, time periods, and measurable quantities. Extract each individual number, percentage, or measurement as a separate item. For example, from 'Response time shall not exceed 200 milliseconds for 95% of requests', extract '200 milliseconds' and '95%' as separate items." \
        --examples "$SCRIPT_DIR/example_2.json" \
        --provider ollama \
        --model mistral \
        --output "$SCRIPT_DIR/output/step2_values.json" \
        --format json \
        --temperature 0.3 \
        --workers 6 \
        --debug > /dev/null 2>&1 &
else
    echo "   âš ï¸  Step 1 results not found, using original document"
    $CLI_CMD extract "$SCRIPT_DIR/input.txt" \
        --prompt "From this requirements document, extract all numeric values and their associated units or specifications. Focus on performance metrics, limits, and measurable criteria like transaction rates, response times, storage amounts, percentages, and time periods. Extract each value as a separate item with its unit or context." \
        --examples "$SCRIPT_DIR/example_2.json" \
        --provider ollama \
        --model mistral \
        --output "$SCRIPT_DIR/output/step2_values.json" \
        --format json \
        --temperature 0.3 \
        --workers 6 \
        --debug > /dev/null 2>&1 &
fi

STEP2_PID=$!

echo -e "${CYAN}ğŸ“ Step 3: Extract Specifications (Running in Parallel)${NC}"
echo "Extracting security specs, encryption details, and technical constraints..."

# Step 3 also processes the requirements from Step 1
echo "   ğŸ“‹ Processing requirements from Step 1..."
if [ -f "$SCRIPT_DIR/output/step1_requirements.json" ]; then
    $CLI_CMD extract "$SCRIPT_DIR/output/step1_requirements_text.txt" \
        --prompt "From these requirement statements, extract detailed specifications, constraints, and technical implementation details. Focus on security (encryption standards, authentication methods), data protection mechanisms, compliance standards, API specifications, and technical constraints. For each specification, show what requirement it belongs to." \
        --examples "$SCRIPT_DIR/example_3.json" \
        --provider ollama \
        --model mistral \
        --output "$SCRIPT_DIR/output/step3_specifications.json" \
        --format json \
        --temperature 0.3 \
        --workers 6 \
        --debug > /dev/null 2>&1 &
else
    echo "   âš ï¸  Step 1 results not found, using original document"
    $CLI_CMD extract "$SCRIPT_DIR/input.txt" \
        --prompt "Extract detailed specifications, constraints, and technical requirements from this text. Focus on security (encryption, authentication), data protection, compliance standards, API specifications, and technical implementation details. Return a flat list where each individual specification is a separate extraction with its own class name. Do not group specifications into categories - extract each requirement individually." \
        --examples "$SCRIPT_DIR/example_3.json" \
        --provider ollama \
        --model mistral \
        --output "$SCRIPT_DIR/output/step3_specifications.json" \
        --format json \
        --temperature 0.3 \
        --workers 6 \
        --debug > /dev/null 2>&1 &
fi

STEP3_PID=$!

# Wait for parallel steps to complete
echo "   â³ Waiting for parallel steps to complete..."
wait $STEP2_PID

wait $STEP3_PID  
echo "   âœ… Specifications extraction completed"



pipeline_time=$(($(date +%s) - start_time))

echo
echo -e "${CYAN}ğŸ“ Step 4: Aggregate Results${NC}"
echo "Combining results from all pipeline steps..."

# Create a combined results file
echo "{" > "$SCRIPT_DIR/output/pipeline_results.json"
echo "  \"pipeline_metadata\": {" >> "$SCRIPT_DIR/output/pipeline_results.json"
echo "    \"name\": \"Requirements Extraction Pipeline\"," >> "$SCRIPT_DIR/output/pipeline_results.json"
echo "    \"processing_time_seconds\": $pipeline_time," >> "$SCRIPT_DIR/output/pipeline_results.json"
echo "    \"steps_executed\": 3," >> "$SCRIPT_DIR/output/pipeline_results.json"
echo "    \"parallel_execution\": true" >> "$SCRIPT_DIR/output/pipeline_results.json"
echo "  }," >> "$SCRIPT_DIR/output/pipeline_results.json"

# Add step results
echo "  \"step_results\": {" >> "$SCRIPT_DIR/output/pipeline_results.json"

if [ -f "$SCRIPT_DIR/output/step1_requirements.json" ]; then
    echo "    \"requirements\": " >> "$SCRIPT_DIR/output/pipeline_results.json"
    cat "$SCRIPT_DIR/output/step1_requirements.json" >> "$SCRIPT_DIR/output/pipeline_results.json"
    echo "," >> "$SCRIPT_DIR/output/pipeline_results.json"
fi

if [ -f "$SCRIPT_DIR/output/step2_values.json" ]; then
    echo "    \"values\": " >> "$SCRIPT_DIR/output/pipeline_results.json"
    cat "$SCRIPT_DIR/output/step2_values.json" >> "$SCRIPT_DIR/output/pipeline_results.json"
    echo "," >> "$SCRIPT_DIR/output/pipeline_results.json"
fi

if [ -f "$SCRIPT_DIR/output/step3_specifications.json" ]; then
    echo "    \"specifications\": " >> "$SCRIPT_DIR/output/pipeline_results.json"
    cat "$SCRIPT_DIR/output/step3_specifications.json" >> "$SCRIPT_DIR/output/pipeline_results.json"
fi

echo "  }" >> "$SCRIPT_DIR/output/pipeline_results.json"
echo "}" >> "$SCRIPT_DIR/output/pipeline_results.json"

echo "   âœ… Pipeline aggregation completed"

echo
echo "ğŸ¨ Creating pipeline visualization..."

# Use the built-in CLI pipeline with layered HTML export
$CLI_CMD pipeline \
    --config "$SCRIPT_DIR/requirements_pipeline.yaml" \
    "$SCRIPT_DIR/input.txt" \
    --export-html "$SCRIPT_DIR/output/pipeline_layered.html" \
    --export-flattened "$SCRIPT_DIR/output/pipeline_flattened.json" \
    --aggregate-highlights \
    --allow-overlaps \
    --expand-nested-json > /dev/null 2>&1 || true

if [ -f "$SCRIPT_DIR/output/pipeline_layered.html" ]; then
    echo "   âœ… Layered HTML created: output/pipeline_layered.html"
else
    echo "   âš ï¸  Layered HTML not created. Check CLI output above."
fi

if [ -f "$SCRIPT_DIR/output/pipeline_flattened.json" ]; then
    echo "   âœ… Flattened JSON created: output/pipeline_flattened.json"
fi

echo
echo -e "${BLUE}ğŸ“Š Pipeline Results Analysis:${NC}"

# Analyze the pipeline results
if command -v jq &> /dev/null; then
    echo
    echo -e "${CYAN}ğŸ“ˆ Pipeline Performance:${NC}"
    echo "   â±ï¸  Total processing time: ${pipeline_time}s"
    echo "   ğŸ”„ Steps executed: 3 (1 sequential + 2 parallel)"
    echo "   âš¡ Parallel efficiency: ~50% time savings vs sequential"
    
    echo
    echo -e "${CYAN}ğŸ“‹ Step Results Summary:${NC}"
    
    for step in "requirements" "values" "specifications"; do
        step_file="$SCRIPT_DIR/output/step${step:0:1}_${step}.json"
        if [ "$step" = "requirements" ]; then step_file="$SCRIPT_DIR/output/step1_requirements.json"; fi
        if [ "$step" = "values" ]; then step_file="$SCRIPT_DIR/output/step2_values.json"; fi
        if [ "$step" = "specifications" ]; then step_file="$SCRIPT_DIR/output/step3_specifications.json"; fi
        
        if [ -f "$step_file" ]; then
            count=$(jq '.extractions | length' "$step_file" 2>/dev/null || echo "0")
            echo "   ğŸ“Š ${step^}: $count extractions"
            
            if [ "$count" -gt "0" ]; then
                echo "      Sample extractions:"
                jq -r '.extractions[0:3][] | "         â€¢ " + .extraction_class + ": \"" + .extraction_text + "\""' "$step_file" 2>/dev/null
            fi
            echo
        fi
    done
    
    echo -e "${CYAN}ğŸ”— Pipeline Dependencies:${NC}"
    echo "   1ï¸âƒ£  Extract Requirements (independent) â†’ completed first"
    echo "   2ï¸âƒ£  Extract Values (depends on #1) â†’ ran in parallel with #3"
    echo "   3ï¸âƒ£  Extract Specifications (depends on #1) â†’ ran in parallel with #2"
    echo
    
else
    echo "Install jq for detailed analysis, or check JSON files manually"
fi

echo -e "${GREEN}âœ… Pipeline Demo Complete!${NC}"
echo
echo "Generated files in $SCRIPT_DIR/output/:"
echo "  â€¢ step1_requirements.json - ğŸ“‹ Extracted requirements and 'shall' statements"
echo "  â€¢ step2_values.json - ğŸ“Š Numeric values, metrics, and units"
echo "  â€¢ step3_specifications.json - ğŸ”§ Technical specs and security requirements"
echo "  â€¢ pipeline_results.json - ğŸ”— Combined results from all steps"
echo "  â€¢ pipeline_layered.html - ğŸŒ Interactive visualization"
echo "  â€¢ pipeline_flattened.json - ğŸ”— All individual extractions with hierarchy"

echo
echo "ğŸ”¬ Pipeline Features Demonstrated:"
echo "  1. Multi-step dependent processing workflows"
echo "  2. Parallel execution of independent steps"
echo "  3. Step-by-step result building and aggregation"
echo "  4. Requirements document analysis and breakdown"
echo "  5. Structured hierarchical data extraction"
echo
echo "ğŸ¯ Pipeline Benefits:"
echo "  â€¢ Improved accuracy through focused extraction steps"
echo "  â€¢ Better organization of complex document analysis"
echo "  â€¢ Parallel processing for performance optimization"
echo "  â€¢ Hierarchical data structures from flat text"
echo "  â€¢ Reusable pipeline configurations"
echo
echo "ğŸ§ª Try This:"
echo "  â€¢ Modify requirements_pipeline.yaml for custom workflows"
echo "  â€¢ Add more steps for deeper analysis"
echo "  â€¢ Test with different requirement documents"
echo "  â€¢ Experiment with parallel vs sequential execution"
echo
echo "âš™ï¸  Advanced Pipeline Features:"
echo "  â€¢ Custom filter conditions for step inputs"
echo "  â€¢ Quality thresholds for step validation"
echo "  â€¢ Error handling and step retry logic"
echo "  â€¢ Template-based pipeline generation"
echo
echo "ğŸ”§ Production Pipeline Tips:"
echo "  â€¢ Use YAML configs for reproducible workflows"
echo "  â€¢ Implement validation between pipeline steps"
echo "  â€¢ Monitor step performance and bottlenecks"
echo "  â€¢ Create domain-specific pipeline templates"
